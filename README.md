# Cross-Document Conflict Detector

A GraphRAG-based Due Diligence tool for Private Equity/M&A sector that identifies logical contradictions across financial/legal documents.

## ğŸ¯ The Problem

In M&A due diligence, analysts spend days manually comparing hundreds of documents to find inconsistencies:
- "Document A says CEO gets **5% equity**"
- "Document B says CEO gets **3% equity**"

Standard LLMs miss these because they process documents separately. **We solve this.**

## ğŸ—ï¸ Architecture

```
Layer 1: Ingestion Engine (The Moat)
â”œâ”€â”€ LlamaParse â†’ Table-aware PDF parsing
â”œâ”€â”€ Semantic Chunking â†’ Preserve clause boundaries
â””â”€â”€ Metadata Extraction â†’ Page numbers, sections

Layer 2: Knowledge Layer (GraphRAG)
â”œâ”€â”€ Vector Store (ChromaDB) â†’ Semantic search
â”œâ”€â”€ Graph Store (NetworkX) â†’ Entity relationships
â”œâ”€â”€ Entity Resolution â†’ Deduplicate aliases
â”œâ”€â”€ Cross-Reference Engine â†’ Find mentions across docs
â””â”€â”€ Timeline Builder â†’ Chronological event tracking

Layer 3: Logic Agents
â”œâ”€â”€ Comparator â†’ Detect value mismatches
â”œâ”€â”€ Judge â†’ Verify and prevent hallucinations
â”œâ”€â”€ Multi-Doc Analyzer â†’ N-way conflict detection
â”œâ”€â”€ Reference Detector â†’ Find missing documents
â””â”€â”€ Report Generator â†’ Executive summaries

Layer 4: Interface
â”œâ”€â”€ FastAPI Backend â†’ 11 REST endpoints
â””â”€â”€ Streamlit UI â†’ Investor-facing demo
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- [Ollama](https://ollama.ai/) (for local LLM) or OpenAI API key
- [LlamaParse API key](https://cloud.llamaindex.ai/) (for table extraction)

### Installation

```bash
# Clone and setup
cd project-F
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Configure environment
cp .env.example .env
# Edit .env with your API keys
```

### Run the Application

```bash
# Terminal 1: Start the API
uvicorn app.main:app --reload

# Terminal 2: Start the UI
streamlit run ui/app.py
```

Then open http://localhost:8501 in your browser.

### Local LLM Setup (Privacy Mode)

```bash
# Install Ollama and pull Llama 3
ollama pull llama3

# Set in .env
LLM_BACKEND=ollama
OLLAMA_MODEL=llama3
```

## ğŸ“ Project Structure

```
project-F/
â”œâ”€â”€ app/                 # FastAPI application (11 endpoints)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ ingestion/       # PDF parsing & chunking
â”‚   â”œâ”€â”€ knowledge/       # Vector/Graph stores, entity resolution
â”‚   â”œâ”€â”€ agents/          # Conflict detection, reports
â”‚   â””â”€â”€ utils/           # LLM factory, logging
â”œâ”€â”€ ui/                  # Streamlit UI (9 components)
â”‚   â”œâ”€â”€ components/      # Upload, Analysis, Conflicts, Graph, Timeline, Report
â”‚   â”œâ”€â”€ utils/           # API client, formatters
â”‚   â””â”€â”€ static/          # CSS styling
â”œâ”€â”€ tests/               # Test suite (184 tests)
â”œâ”€â”€ data/                # Uploads, processed files, graphs
â””â”€â”€ scripts/             # CLI utilities
```

## ğŸ¨ UI Features

- **Document Upload**: Drag-and-drop PDF upload with progress tracking
- **Data Inspector**: Side-by-side PDF vs parsed output view
- **Analysis Dashboard**: Live reasoning trace with audit log
- **Conflict Workbench**: Master-detail view with PDF citations
- **Knowledge Graph**: Interactive PyVis visualization
- **Timeline View**: Chronological events with conflict highlighting
- **Executive Summary**: Downloadable markdown reports

## ğŸ”Œ API Endpoints

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/health` | GET | Health check |
| `/ingest` | POST | Upload and process PDF |
| `/detect-conflicts` | POST | Pairwise conflict detection |
| `/analyze` | POST | Multi-document analysis |
| `/timeline` | POST | Build event timeline |
| `/search` | GET | Entity search |
| `/report` | POST | Generate executive summary |
| `/graph` | GET | Graph data JSON |
| `/graph/html` | GET | Interactive graph HTML |
| `/missing-documents` | POST | Find referenced but not uploaded docs |

## ğŸ”’ Privacy First

The system is designed for on-premise deployment:
- Swap LLM backend in one line of config
- Local embeddings with HuggingFace models
- All data stays on your infrastructure

## ğŸ“„ License

MIT
